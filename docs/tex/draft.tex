\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
%\usepackage[square]{natbib}
\usepackage{color}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{Pareto-Optimal Multistate Protein Design using Tree Graphical Models}
\author{Subhodeep Moitra \\ {\tt subhodee@andrew.cmu.edu}}

\maketitle

\begin{abstract}
The problem of protein design is to find a protein sequence which folds into a given protein structure. Multistate protein design generalizes this to find a protein sequence which folds into multiple protein states. This involves finding a sequence that is not only stable in one state but in the other state as well. This problem can be posed as a multi-objective optimization problem. By making suitable modelling approximations using graphical models, I propose to solve this multi-state design problem that allows the discovery of a set of sequences that are pareto optimal. Pareto-optimality in this context refers to the property that it is not possible to improve stability in one state without hurting the stability of the other state. Theory  , experiments and conclusions will be presented. 
\end{abstract}

\clearpage
\newpage

\section{Introduction}
To be added shortly \cite{Ambroggio2006}.

\section{Method}
In this section, we describe and outline aspects of the method. The key idea is that being able to find the minimum energy sequence using dynamic programming(DP) allows us to find pareto optimal solution sequences for all convex combinations of energy functions of multi-states. Thus, we want to cleverly map sequence to an energy value using a function that can be solved using DP.

\subsection{Hidden Markov Model(HMM)}
An example of an energy function that can be solved using DP is the popular Hidden Markov Model(HMM). The HMM is probabilistic model represented as a chain of hidden markov states where each state has an associated observed variable. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/hmm.pdf}
    \caption{A canonical HMM}
    \label{hmm1}
\end{figure}


The graphical representation of the HMM factorizes according to the following probability distribution :

\begin{equation}
P(X_1,\ldots,X_n|\mathbf{O_1},\ldots,\mathbf{O_n}) = P(X_1)\prod_{i=1}^{n}{P(\mathbf{O_i}|X_i)}\prod_{i=1}^{n-1}{P(X_{i+1}|X_i)}
\end{equation}

Note, that the chain structure gives rise to a distribution where the dependence between random variables $X_1 to X_n$ is only through adjacent random variables $X_i and X_{i+1}$. This interesting decomposition allows us to find the minimum energy/maximum probability sequence efficiently using DP. The DP solution to a HMM is the famous Viterbi Decoding algorithm. In particular, we want the solution of :

\begin{align*}
X^\star_{1:n} &= \argmax_{X_1,\ldots ,X_n} P(X_1,\ldots,X_n|\mathbf{O_1},\ldots,\mathbf{O_n})\\
  &= \argmax_{X_1,\ldots ,X_n}P(X_1)\prod_{i=1}^{n}{P(\mathbf{O_i}|X_i)}\prod_{i=1}^{n-1}{P(X_{i+1}|X_i)}
\end{align*}


The main steps in the Viterbi algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    P(\mathbf{O_1}|X_1{=}k)P(X_1{=}k) & \quad \text{if $i=1$}\\
    P(\mathbf{O_i}|X_i{=}k)\max_{j}\{P(X_{i}{=}k|X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum probability value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Chain Structured Markov Random Field (CMRF)}
We will consider a generalization of the HMM, to an undirected graphical model. This is known as the chain structured Markov Random Field (CMRF). The purpose of generalizing to an MRF is that it gives more flexibility in the form of the energy functions and also that there are more general inference algorithms that can be used. In particular, the Max-Product algorithm can be used to solve the CMRF. The CMRF replaces the Conditional probability tables(CPTs) present in the HMM with node and edge potentials. Figure~\ref{fig:CMRF} describes a CMRF. The max-product algorithm in the case of the CMRF is actually very similar to the dynamic programming solution of the HMM. 



\begin{figure}[h]
% selecting unit length
\centering
% used for centering Figure
\setlength{\unitlength}{0.3cm}
\begin{picture}(40,20)(0,0)
\multiput(5,15)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{3.7}}
\thicklines
\multiput(7,15)(8,0){2}{\line(1,0){4}}
\multiput(20,15)(1,0){4}{\circle*{0.3}}
\multiput(5,13)(8,0){2}{\line(0,-1){4}}
\put(24,15){\line(1,0){4}}
\put(30,13){\line(0,-1){4}}
\thinlines
\put(30,15){\circle{4}}
\put(30,7){\circle{4}}
\put(30,7){\circle{3.7}}
\put(3.5,14.6){$\phi(X_1)$}
\put(11.5,14.6){$\phi(X_2)$}
\put(28.5,14.6){$\phi(X_n)$}
\put(3.5,6.6){$\phi(O_1)$}
\put(11.5,6.6){$\phi(O_2)$}
\put(28.5,6.6){$\phi(O_n)$}
\put(7,16.5){\footnotesize{$\psi(X_1{,}X_2)$}}
\put(15,16.5){\footnotesize{$\psi(X_2{,}X_3)$}}
\put(22.5,16.5){\footnotesize{$\psi(X_{n{-}1}{,}X_n)$}}
\put(5.2,10.6){\footnotesize{$\psi(X_1{,}O_1)$}}
\put(13.2,10.6){\footnotesize{$\psi(X_2{,}O_2)$}}
\put(30.2,10.6){\footnotesize{$\psi(X_n{,}O_n)$}}
\end{picture}
\label{fig:CMRF}
\caption{A chain structured Markov Random Field. Probability of a sequence assignment is defined as a function of the node and edge potentials}
\end{figure}

\pagebreak

The probability of the a sequence assignment is defined as follows :
\begin{align*}
P(X_1,X_2,\dotsc,X_n) &= \frac{1}{Z}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_1})\psi(\mathbf{O_1},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})} \\
\text{where} \quad Z &= \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_1})\psi(\mathbf{O_1},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\end{align*}

%\includegraphics[width=\textwidth]{dummy.pdf}

$Z$ is a normalizing constant known as the partition function. The value of $Z$ can be calculated efficiently using the sum-product algorithm. To calculate the maximum probability assignment $X^*$, we don't even need $Z$. $X^*$ can be evaluated using a dynamic programming formulation known as the max product algorithm. 
\begin{align*}
X^* &= \argmax_{\mathbf{X}}P(\mathbf{X})\\
 &= \argmax_{X_1,X_2,\dotsc,X_n}\frac{1}{Z}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_1})\psi(\mathbf{O_1},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})} \\
 &= \argmax_{X_1,X_2,\dotsc,X_n}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_1})\psi(\mathbf{O_1},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}
\end{align*}

 The main steps in the max-product algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    \phi(\mathbf{O_1})\psi(\mathbf{O_1},X_1{=}k)\phi(X_1{=}k) & \quad \text{if $i=1$}\\
    \phi(\mathbf{O_i})\phi(X_i{=}k)\psi(\mathbf{O_i},X_i{=}k)\max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum functional value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 


\subsection{Convex combinations of Energy Functions}
To be added shortly
%\begin{figure}
%    \centering
%    \includegraphics[width=3.0in]{myfigure}
%    \caption{Simulation Results}
%    \label{simulationfigure}
%\end{figure}

\section{Conclusion}
Write your conclusion here.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}