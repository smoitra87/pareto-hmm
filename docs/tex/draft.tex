\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
%\usepackage[square]{natbib}
\usepackage{color}
\usepackage{algpseudocode}
\usepackage{algorithm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\title{Pareto-Optimal Multistate Protein Design using Tree Graphical Models}
\author{Subhodeep Moitra \\ {\tt subhodee@andrew.cmu.edu}}

\maketitle

\begin{abstract}
The problem of protein design is to find a protein sequence which folds into a given protein structure. Multistate protein design generalizes this to find a protein sequence which folds into multiple protein states. This involves finding a sequence that is not only stable in one state but in the other state as well. This problem can be posed as a multi-objective optimization problem. By making suitable modelling approximations using graphical models, I propose to solve this multi-state design problem that allows the discovery of a set of sequences that are pareto optimal. Pareto-optimality in this context refers to the property that it is not possible to improve stability in one state without hurting the stability of the other state. Theory  , experiments and conclusions will be presented. 
\end{abstract}


\section{Introduction}
To be added shortly \cite{Ambroggio2006}.

\section{Method}
In this section, we describe and outline aspects of the method. The key idea is that being able to find the minimum energy sequence using dynamic programming(DP) allows us to find pareto optimal solution sequences for all convex combinations of energy functions of multi-states. Thus, we want to cleverly map sequence to an energy value using a function that can be solved using DP.

\subsection{Hidden Markov Model(HMM)}
An example of an energy function that can be solved using DP is the popular Hidden Markov Model(HMM). The HMM is probabilistic model represented as a chain of hidden markov states where each state has an associated observed variable. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/hmm.pdf}
    \caption{A canonical HMM}
    \label{hmm1}
\end{figure}


The graphical representation of the HMM factorizes according to the following probability distribution :

\begin{equation}
P(X_1,\ldots,X_n|\mathbf{O_1},\ldots,\mathbf{O_n}) = P(X_1)\prod_{i=1}^{n}{P(\mathbf{O_i}|X_i)}\prod_{i=1}^{n-1}{P(X_{i+1}|X_i)}
\end{equation}

Note, that the chain structure gives rise to a distribution where the dependence between random variables $X_1 to X_n$ is only through adjacent random variables $X_i and X_{i+1}$. This interesting decomposition allows us to find the minimum energy/maximum probability sequence efficiently using DP. The DP solution to a HMM is the famous Viterbi Decoding algorithm. In particular, we want the solution of :

\begin{align*}
X^\star_{1:n} &= \argmax_{X_1,\ldots ,X_n} P(X_1,\ldots,X_n|\mathbf{O_1},\ldots,\mathbf{O_n})\\
  &= \argmax_{X_1,\ldots ,X_n}P(X_1)\prod_{i=1}^{n}{P(\mathbf{O_i}|X_i)}\prod_{i=1}^{n-1}{P(X_{i+1}|X_i)}
\end{align*}


The main steps in the Viterbi algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    P(\mathbf{O_1}|X_1{=}k)P(X_1{=}k) & \quad \text{if $i=1$}\\
    P(\mathbf{O_i}|X_i{=}k)\max_{j}\{P(X_{i}{=}k|X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum probability value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Chain Structured Markov Random Field (CMRF)}
We will consider a generalization of the HMM, to an undirected graphical model. This is known as the chain structured Markov Random Field (CMRF). The purpose of generalizing to an MRF is that it gives more flexibility in the form of the energy functions and also that there are more general inference algorithms that can be used. In particular, the Max-Product algorithm can be used to solve the CMRF. The CMRF replaces the Conditional probability tables(CPTs) present in the HMM with node and edge potentials. Figure~\ref{fig:CMRF} describes a CMRF. The max-product algorithm in the case of the CMRF is actually very similar to the dynamic programming solution of the HMM. 



\begin{figure}[h]
% selecting unit length
\centering
% used for centering Figure
\setlength{\unitlength}{0.3cm}
\begin{picture}(40,20)(0,0)
\multiput(5,15)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{3.7}}
\thicklines
\multiput(7,15)(8,0){2}{\line(1,0){4}}
\multiput(20,15)(1,0){4}{\circle*{0.3}}
\multiput(5,13)(8,0){2}{\line(0,-1){4}}
\put(24,15){\line(1,0){4}}
\put(30,13){\line(0,-1){4}}
\thinlines
\put(30,15){\circle{4}}
\put(30,7){\circle{4}}
\put(30,7){\circle{3.7}}
\put(3.5,14.6){$\phi(X_1)$}
\put(11.5,14.6){$\phi(X_2)$}
\put(28.5,14.6){$\phi(X_n)$}
\put(3.5,6.6){$\phi(O_1)$}
\put(11.5,6.6){$\phi(O_2)$}
\put(28.5,6.6){$\phi(O_n)$}
\put(7,16.5){\footnotesize{$\psi(X_1{,}X_2)$}}
\put(15,16.5){\footnotesize{$\psi(X_2{,}X_3)$}}
\put(22.5,16.5){\footnotesize{$\psi(X_{n{-}1}{,}X_n)$}}
\put(5.2,10.6){\footnotesize{$\psi(X_1{,}O_1)$}}
\put(13.2,10.6){\footnotesize{$\psi(X_2{,}O_2)$}}
\put(30.2,10.6){\footnotesize{$\psi(X_n{,}O_n)$}}
\end{picture}
\label{fig:CMRF}
\caption{A chain structured Markov Random Field. Probability of a sequence assignment is defined as a function of the node and edge potentials}
\end{figure}

\pagebreak

The probability of the a sequence assignment is defined as follows :
\begin{align*}
P(X_1,X_2,\dotsc,X_n) &= \frac{1}{Z}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})} \\
\text{where} \quad Z &= \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\end{align*}

%\includegraphics[width=\textwidth]{dummy.pdf}

$Z$ is a normalizing constant known as the partition function. The value of $Z$ can be calculated efficiently using the sum-product algorithm. To calculate the maximum probability assignment $X^*$, we don't even need $Z$. $X^*$ can be evaluated using a dynamic programming formulation known as the max product algorithm. 
\begin{align*}
X^* &= \argmax_{\mathbf{X}}P(\mathbf{X})\\
 &= \argmax_{X_1,X_2,\dotsc,X_n}\frac{1}{Z}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})} \\
 &= \argmax_{X_1,X_2,\dotsc,X_n}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}
\end{align*}

 The main steps in the max-product algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    \phi(\mathbf{O_1})\psi(\mathbf{O_1},X_1{=}k)\phi(X_1{=}k) & \quad \text{if $i=1$}\\
    \phi(\mathbf{O_i})\phi(X_i{=}k)\psi(\mathbf{O_i},X_i{=}k)\max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum functional value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Multistate protein design}
The problem of protein structure prediction is finding the lowest energy structure given a particular sequence. Protein design is known as the inverse protein structure prediction problem and involves finding the lowest energy sequence for a given structure. Multistate protein design generalizes protein design by asking to find a sequence that is stable for multiple structures. This is often posed as a  multi-objective optimization problem~\cite{Zheng2009}.

\paragraph*{} Suppose, $E_A(X)$ and $E_B(X)$ are the energies of a sequence in state A and state B respectively. Then one way of doing multistate design would be to find $X^* = \arg \min_{X} E_A(X) + E_B(X)$. This would be the sequence that is likely to be stable under both state A and state B. Building on this idea, we might prefer to find a sequence $X^*$ that is more stable in one form than the other. We can thus solve for $X^* = \arg \min_{X} \theta_AE_A(X) + \theta_BE_B(X)$ where $\theta_A+\theta_B=1$ and $\theta_A,\theta_B \geq 0$. This sequence might be different from the solution to the previous optimization problem. By trying different combinations of $\theta_A$ and $\theta_B$ it is possible to find a number of different low energy sequences. However, it is not possible to try all possible values of $\theta$, nor is it possible to know which ones to pick to get a variety of different sequences.

\paragraph*{} We will present an algorithm for finding optimal sequences for all combinations of $\theta_A$ and $\theta_B$. This is known as the pareto optimal frontier of the sequence space. See Figure~\ref{fig:cvxfront} for an illustration of the pareto-optimal frontier. This frontier is pareto-optimal in the sense that it is not possible to unilaterally improve the energy function value of one state without adversely affecting the energy value of the other state. Thus each sequence on the frontier is optimal for certain sets of $\theta_A$ and $\theta_B$. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/convexfront.pdf}
    \caption{This figure illustrates the concept of a pareto-optimal convex frontier. The red curve is defined as the pareto-frontier and represents the tradeoff between $E_A(X)$ and $E_B(X)$. P1 and P2 are two points on the pareto frontier. P1 is better in terms of Energy A and P2 is better in terms of Energy B but not vice versa, thus showing a trade-off. The region D2(blue rectangle) is strictly dominated by P2, the region D1(green rectangle) is dominated by P1 and the region D12(yellow) is dominated by both P1 and P2. Domination means that points in these regions are clearly worse in both Energy A and Energy B. Also, notice the perpendicular drawn from the green dashed line and blue dashed line meets the pareto-frontier at P1 and P2 respectively.}
    \label{fig:cvxfront}
\end{figure}

\pagebreak

\subsubsection*{Quick hull procedure}
The Quick hull procedure is used to find all the points on the pareto optimal frontier. The quick hull is traditionally used to find the convex hull of a set of points. In our case we are only interested in the lower left convex frontier similar to ~\cite{Zheng2009}.  The algorithm in pseudo code is described in Algorithm~\ref{algo:quickhull1}. The basic idea behind the algorithm is to find a pair of points on the pareto optimal frontier. Find the point farthest from the line joining the two points and then to repeat the procedure with this newly found point and the previous two points. When there are no newer points to find, all points on the pareto optimal frontier have been discovered. 

\begin{algorithm}
\caption{Quick Hull} \label{algo:quickhull1}
\begin{algorithmic}
\State Initialize queue $Q \gets \emptyset$
\State Find $X_A \gets \arg \min_X E_A(X) $
\State Find $X_B \gets \arg \min_X E_B(X) $
\State Enqueue $(X_A,X_B)$
\State $C_H \gets \{X_A,X_B\}$
\Repeat
	\State Dequeue from Q a pair $(X_1,X_2)$ and assert($E_A(X_1) < E_A(X_2)$)
	\State $m \gets \frac{E_B(X_1)-E_B(X_2)}{E_A(X_1)-E_A(X_2)}$
	\State $\theta_A \gets \frac{-m}{1-m}$
	\State $\theta_B \gets \frac{1}{1-m}$
	\State Find $X_{AB} \gets \arg \min_X \theta_A E_A(X) + \theta_B E_B(X) $
	\If{$X_{AB} \neq X_A$ and $X_{AB} \neq X_B$ }
		\State $C_H \gets C_H \cup \{X_{AB}\}$
		\State Enqueue $(X_A,X_{AB})$ and $(X_{AB},X_B)$
	\EndIf
\Until{Q is empty}
\State \Return $C_H$
\end{algorithmic}
\end{algorithm}


\subsection{Convex combinations of CMRFs}
For multi-state protein design we can combine the energy functions as follows.
\[
E_{AB}(X) = \theta_A E_A(X) + \theta_B E_B(X) \quad \text{where} \quad \theta_A,\theta_B \geq 0 \text{ and } \theta_A + \theta_B =1
\]
Here $E_A(X)$ is any energy function mapping sequence to energy values for state A. The lower the energy the better the fit for sequence X to state A. $E_A(X)$ can also be written as a CMRF.
\[
E_A(X) = -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
Note that $-\log$ is to formulate the problem as a minimization problem so that it agrees with the energy concept. However the optimal sequence remain the same. 
\[
\begin{split}
X_A &= \arg \min_X -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]\\
&= \arg \max_X \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\end{split}
\]
Taking the $-\log$ also offers benefits as it lets us combine convex combinations of CMRF energy functions and gives rise to new graphical models called Tree MRFS (TMRFs). These TMRFs are conceptually very similar to CMRFs and possess the properties of efficient inference and decoding. 

\begin{align*}
X_A &= \arg \min_X E_A(X) \\
&= \arg \min_X -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]\\
&= \arg \min_X -\left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\phi(\mathbf{O^A_i})+\sum_{i=1}^{n} \log\psi(\mathbf{O^A_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right] \\
&= \arg \min_X -\left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^A_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right]
\end{align*}

Similarly,
\begin{align*}
X_B &= \arg \min_X E_B(X) \\
&= \arg \min_X -\left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^B_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right]
\end{align*}
\\
Taking the convex combination of the energy functions we get 
\begin{align*}
X_{AB} &= \arg \min_X \theta_A E_A(X) + \theta_B E_B(X) \\
\text{s.t} &\quad  \theta_A + \theta_B  = 1 \\
& \theta_A, \theta_B \geq 0
\end{align*}
Expanding $E_A(X)$ and $E_B(X)$ we get, 
\begin{align*}
X_{AB} &= \arg \min_X -\theta_A \left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^A_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right] \\
&-\theta_B \left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^B_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right] \\
&= \arg \min_X -\left( \sum_{i=1}^{n}{\log\phi(X_i)} + \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}\right)\\ 
 &-\left( \sum_{i=1}^{n}{(\theta_A\log\psi(\mathbf{O^A_i},X_i) + \theta_B\log\psi(\mathbf{O^B_i},X_i))} \right) \\
 &= \arg \min_X -\left( \sum_{i=1}^{n}{\log\phi(X_i)} + \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}\right)\\ 
 &-\left( \sum_{i=1}^{n}{(\log\psi(\mathbf{O^A_i},X_i)^{\theta_A} + \log\psi(\mathbf{O^B_i},X_i)^{\theta_B})} \right)
\end{align*}

This looks very much like decoding on a CMRF with the added change that the emission edge potential values are exponentiated. In fact this distribution factorizes according to the TMRF shown in Figure~\ref{fig:CMRF2}.



\begin{figure}[h!]
% selecting unit length
\centering
% used for centering Figure
\setlength{\unitlength}{0.3cm}
\begin{picture}(40,25)(0,0)
\multiput(5,15)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{3.7}}
\multiput(5,23)(8,0){2}{\circle{4}}
\multiput(5,23)(8,0){2}{\circle{3.7}}
\thicklines
\multiput(7,15)(8,0){2}{\line(1,0){4}}
\multiput(20,15)(1,0){4}{\circle*{0.3}}
\multiput(5,13)(8,0){2}{\line(0,-1){4}}
\multiput(5,17)(8,0){2}{\line(0,1){4}}
\put(24,15){\line(1,0){4}}
\put(30,13){\line(0,-1){4}}
\put(30,17){\line(0,1){4}}
\thinlines
\put(30,15){\circle{4}}
\put(30,7){\circle{4}}
\put(30,7){\circle{3.7}}
\put(30,23){\circle{4}}
\put(30,23){\circle{3.7}}
\put(3.5,14.6){$\phi(X_1)$}
\put(11.5,14.6){$\phi(X_2)$}
\put(28.5,14.6){$\phi(X_n)$}
\put(3.5,6.6){$\phi(O^A_1)$}
\put(11.5,6.6){$\phi(O^A_2)$}
\put(28.5,6.6){$\phi(O^A_n)$}
\put(3.5,22.6){$\phi(O^B_1)$}
\put(11.5,22.6){$\phi(O^B_2)$}
\put(28.5,22.6){$\phi(O^B_n)$}
\put(7,16.5){\footnotesize{$\psi(X_1{,}X_2)$}}
\put(15,16.5){\footnotesize{$\psi(X_2{,}X_3)$}}
\put(22.5,16.5){\footnotesize{$\psi(X_{n{-}1}{,}X_n)$}}
\put(5.2,10.6){\footnotesize{$\psi^{\theta_A}(X_1{,}O^A_1)$}}
\put(13.2,10.6){\footnotesize{$\psi^{\theta_A}(X_2{,}O^A_2)$}}
\put(30.2,10.6){\footnotesize{$\psi^{\theta_A}(X_n{,}O^A_n)$}}
\put(5.2,18.6){\footnotesize{$\psi^{\theta_B}(X_1{,}O^B_1)$}}
\put(13.2,18.6){\footnotesize{$\psi^{\theta_B}(X_2{,}O^B_2)$}}
\put(30.2,18.6){\footnotesize{$\psi^{\theta_B}(X_n{,}O^B_n)$}}
\end{picture}
\label{fig:CMRF2}
\caption{A Tree structured Markov Random Field that is used for modelling multi-state protein design. This Tree structured MRF results from taking a convex combination $\theta_A E_A(X) + \theta_B E_B(X)$ of the log likelihoods of CMRFs for State A and State B }
\end{figure}

\pagebreak

\subsection{Max-Decoding on TMRFs}
In the previous section we showed how the convex combination of negative log likelihoods of CMRFs can be used to construct a TMRF. Here we will show how efficient decoding can be performed on the TMRF using the similar variant of Viterbi decoding.  

\begin{align*}
X_{AB} &= \argmin_X \left( \theta_A E_A(X) + \theta_B E_B(X)\right) \\
&= \arg \min_X \left( -\sum_{i=1}^{n}{\log\phi(X_i)} + \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}\right)\\ 
 &-\left( \sum_{i=1}^{n}{(\log\psi(\mathbf{O^A_i},X_i)^{\theta_A} + \log\psi(\mathbf{O^B_i},X_i)^{\theta_B})} \right) \\
 &= \argmax_{X}\log\left( \prod_{i=1}^{n}{\phi(X_i)\psi(\mathbf{O^A_i},X_i)^{\theta_A} \psi(\mathbf{O^B_i},X_i)^{\theta_B}}\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}\right) \\
 &= \argmax_{X}\left( \prod_{i=1}^{n}{\phi(X_i)\psi(\mathbf{O^A_i},X_i)^{\theta_A} \psi(\mathbf{O^B_i},X_i)^{\theta_B}}\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}\right)
\end{align*}

 The main steps in the max-product algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    \psi(\mathbf{O^A_1},X_1{=}k)^{\theta_A}\psi(\mathbf{O^B_1},X_1{=}k)^{\theta_B}\phi(X_1{=}k) & \quad \text{if $i=1$}\\
    \phi(X_i{=}k)\psi(\mathbf{O^A_i},X_i{=}k)^{\theta_A}\psi(\mathbf{O^B_1},X_1{=}k)^{\theta_B}\cdots\\
    \max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum functional value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O^A_{1:i}}$ and $\mathbf{O^B_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Convex Hull Procedure on TMRFs}
In the previous section we showed how to find $X_{AB} = \argmin_X \left( \theta_A E_A(X) + \theta_B E_B(X)\right)$, for any convex combination with $\theta_A$ and $\theta_B$.In this section, we will put it all together to define a comprehensive algorithm that can be used for finding the entire pareto frontier for sequence design tasks.


\subsubsection{Normalizing CMRFs}
So far we have been assuming that the $E_A(X)$ and $E_B(X)$ lies in the positive quadrant. However this assumption need not not be true under our CMRF model. This is because it is possible that $f(X) > 1$ and conversely $-\log f(X) < 0$. This can lead to the energy function values being negative as shown in Figure~\ref{fig:cvxfront2}. We can potentially fix this be normalizing energy the Likelihood function with the partition function Z. The partitions can be efficiently calculated using the sum-product algorithm for trees. However we will show that this step is not needed. 

For e.g. let the partition functions for each of the states be.
\[
Z_A = \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
and 
\[
Z_B = \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^B_i})\psi(\mathbf{O^B_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
\\
If instead of the regular energy function 
\[
E_A(X) = -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
we normalized the energy function with $Z_A$
\[
\begin{split}
E_A'(X) &= -\log  \frac{1}{Z_A} \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]\\
 &= -\log   \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right] + \log Z_A \\
	&= E_A(X) + \log Z_A
\end{split}
\]
\\
Then optimizing the multistate Energy function would amount to : 
\[
\begin{split}
X^{\dagger} &= \arg \min_X \left( \theta_A E_A'(X)  + \theta_B E_B'(X) \right) \\
 &= \arg \min_X \left( \theta_A E_A(X) + \theta_B E_B(X) + \theta_A \log Z_A + \theta_B \log Z_B \right) \\
 &= \arg \min_X \left( \theta_A E_A(X) + \theta_B E_B(X) \right) \\
  &= X^*
\end{split}
\]

This shows that normalizing the energy function values by $Z_A$ and $Z_B$ is not necessary in order to find point on the pareto optimal frontier. This is further demonstrated in Figure~\ref{fig:cvxfront2}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/convexfront2.pdf}
    \caption{This figure illustrates the issue with normalizing. When dealing with negative log likelihood values, it is possible that pareto frontier may not lie in the positive quadrant. This however does not imply that the method fails. Normalizing with the partition function merely corresponds to translating the points in the energy space. This transformation keeps the optimal solution invariant.  }
    \label{fig:cvxfront2}
\end{figure}
\pagebreak


\subsubsection{Clarification on Energy Convex Combination}
From the discussion in the beginning, the reader may have been lead to believe that we are taking convex combinations of the likelihood of multiple states and then maximizing this likelihood. While in spirit this is true, however practically we are optimizing something else. Let $L(X|O^A)$  and $L(X|O^B)$ be the likelihoods of the sequences under state A and state B respectively. The reader may have been lead to believe that we are solving the following problem
\begin{equation}
\label{eqn:clarif1}
X^* = \arg \max_X \left( \theta_A L(X|O^A) + \theta_B L(X|O^B) \right)
\end{equation}
But in, reality we are solving : 
\begin{align}
X^* &= \arg \min_X \left( -\theta_A \log L(X|O^A) - \theta_B \log L(X|O^B) \right) \\
&= \arg \min_X \left( \log \frac{1}{L(X|O^A)^{\theta_A} L(X|O^B)^{\theta_B}} \right) \\
&= \arg \max_X \left( L(X|O^A)^{\theta_A} L(X|O^B)^{\theta_B} \right)
\label{eqn:clarif2} 
\end{align}

Clearly Eqn~\ref{eqn:clarif2} is not the same as Eqn~\ref{eqn:clarif1}. One is minimizing the arithmetic mean, while the other is minimizing the geometric mean. It remains to be seen what the semantic consequence of this mathematical detail is. 

\section{Conclusion}
Write your conclusion here.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}