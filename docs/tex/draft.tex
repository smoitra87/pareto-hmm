\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
%\usepackage[square]{natbib}
\usepackage{color}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\title{Pareto-Optimal Multistate Protein Design using Tree Graphical Models}
\author{Subhodeep Moitra \\ {\tt subhodee@andrew.cmu.edu}}

\maketitle

\begin{abstract}
The problem of protein design is to find a protein sequence which folds into a given protein structure. Multistate protein design generalizes this to find a protein sequence which folds into multiple protein states. This involves finding a sequence that is not only stable in one state but in the other state as well. This problem can be posed as a multi-objective optimization problem. By making suitable modelling approximations using graphical models we attempt to solve this optimization problem exactly and efficiently. We also show how to efficiently find all pareto-optimal sequences by enumerating the pareto-optimal frontier in sequence space. Thus, we make a novel contribution to the repertoire of algorithms for protein design. 
\end{abstract}


\section{Introduction}
A major goal in medicine is to be able to design biological macromolecules that perform a certain desired biological function. These biological macromolecules called proteins are ubiquitously found in all living organisms. They perform a number of functions ranging from motion, structure, signalling, catalysis, etc. They are the molecular machines of the biological world. One of the primary reasons that proteins are able to perform so many diverse functions is that individual proteins can adopt multiple roles. They do so by being able to show great structural diversity. While it is certainly true that sequence determines structure, it is not true that it is a single static structure. Proteins can adopt multiple structures and conformations. Some proteins are particularly better than others in their ability to adopt distinct conformations. We refer to these proteins as multistate proteins. 
\\
\\
The problem of protein design is to find a protein sequence which folds into a given protein structure. Multistate protein design generalizes this to find a protein sequence which folds into multiple protein states. This involves finding a sequence that is not only stable in one state but in the other state as well. Biologist have previously designed proteins that can switch between folded and disordered states, between distinct folded states and between different aggregation states~\cite{Ambroggio2006}. These proteins switch under a variety of trigger mechanisms such as pH shifts, post-translational modification and ligand binding. However most efforts lead by biologists have heavily relied on detailed prior understanding of the system and trial and error. 
\\
\\
Computational attempts at multistate design have been stymied by the enormous complexity of the protein design space. Pierce et.al~\cite{Pierce2002} proved that protein design is NP-hard. Multistate design a generalization of protein design is by extension NP-hard as well. Previous attempts at multistate protein design have involved a lot of different strategies including Markov Chain Monte Carlo (MCMC)~\cite{Leaver-Fay2011,Ambroggio2006}, Genetic Algorithms, Dead End Elimination~\cite{Yanover2007} and even graphical models~\cite{Fromer2010}.The multistate applications have ranged from negative design, protein-protein binding, homodimerization, allosteric design, etc. Each of these methods make significant contributions to the field of multistate protein design. However each of them suffers from some drawback or the other. MCMC and GA methods are approximate and do not have any guarantees and are expensive. DEE is an exact method, but is very expensive to calculate and cannot be scaled up. BP using MRFs is the most elegant yet, but it also suffers from computational cost and scale-up issues. 
\\
\\
When designing algorithms, there is often a trade-off between accuracy and speed. In this paper, I will present a method that is fast and exact. It is very flexible allowing the user to supply various features. It also allows for full enumeration of the pareto-frontier of the sequence space. Unfortunately, these benefits come at the cost of accuracy and expressive power. Specifically, it makes an sequential dependency assumption between the amino acids of a protein. This is obviously not true since proteins are 3D structures and can fold back in order to interact with non-sequential parts of itself. While it is true that this is a limitation, it remains to be seen how much of a limitation it really is. Many chain methods such as HMMs are very successfully used within the biological community. 
\\
\\
Our method uses tree-structured MRFs(TMRFs)for efficient inference and decoding to find the minimum energy sequence for a particular state. We make a clever observation that minimum energy sequences of convex combinations of states can be found efficiently as well when using a TMRF. This unique property lets us fully enumerate the pareto-frontier of the sequence space thus identifying all the sequences that are strictly dominant to other sequences. Thus it lets us very quickly describe the sequence space. In the rest of the paper we will present the theory behind all the methods. We first introduce Hidden Markov Models (HMMs) for motivation. We then move onto chain structured CMRFs and talk about efficient max-decoding in these frameworks. We further generalize CMRFs to tree structured MRFs (TMRFs) and show how to do max decoding under this setting as well. Then we tie all the models together using the quick hull algorithm for enumerating the sequences in the pareto frontier. Finally, we close by talking about some mathematical subtleties of the proposed method.

\section{Method}
In this section, we describe and outline aspects of the method. The key idea is that being able to find the minimum energy sequence using dynamic programming(DP) allows us to find pareto optimal solution sequences for all convex combinations of energy functions of multi-states. Thus, we want to cleverly map sequence to an energy value using a function that can be solved using DP.

\subsection{Hidden Markov Model(HMM)}
An example of an energy function that can be solved using DP is the popular Hidden Markov Model(HMM). The HMM is probabilistic model represented as a chain of hidden markov states where each state has an associated observed variable. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/hmm.pdf}
    \caption{A canonical HMM}
    \label{hmm1}
\end{figure}


The graphical representation of the HMM factorizes according to the following probability distribution :

\begin{equation}
P(X_1,\ldots,X_n|\mathbf{O_1},\ldots,\mathbf{O_n}) = P(X_1)\prod_{i=1}^{n}{P(\mathbf{O_i}|X_i)}\prod_{i=1}^{n-1}{P(X_{i+1}|X_i)}
\end{equation}

Note, that the chain structure gives rise to a distribution where the dependence between random variables $X_1 to X_n$ is only through adjacent random variables $X_i and X_{i+1}$. This interesting decomposition allows us to find the minimum energy/maximum probability sequence efficiently using DP. The DP solution to a HMM is the famous Viterbi Decoding algorithm. In particular, we want the solution of :

\begin{align*}
X^\star_{1:n} &= \argmax_{X_1,\ldots ,X_n} P(X_1,\ldots,X_n|\mathbf{O_1},\ldots,\mathbf{O_n})\\
  &= \argmax_{X_1,\ldots ,X_n}P(X_1)\prod_{i=1}^{n}{P(\mathbf{O_i}|X_i)}\prod_{i=1}^{n-1}{P(X_{i+1}|X_i)}
\end{align*}


The main steps in the Viterbi algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    P(\mathbf{O_1}|X_1{=}k)P(X_1{=}k) & \quad \text{if $i=1$}\\
    P(\mathbf{O_i}|X_i{=}k)\max_{j}\{P(X_{i}{=}k|X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum probability value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Chain Structured Markov Random Field (CMRF)}
We will consider a generalization of the HMM, to an undirected graphical model. This is known as the chain structured Markov Random Field (CMRF). The purpose of generalizing to an MRF is that it gives more flexibility in the form of the energy functions and also that there are more general inference algorithms that can be used. In particular, the Max-Product algorithm can be used to solve the CMRF. The CMRF replaces the Conditional probability tables(CPTs) present in the HMM with node and edge potentials. Figure~\ref{fig:CMRF} describes a CMRF. The max-product algorithm in the case of the CMRF is actually very similar to the dynamic programming solution of the HMM. 



\begin{figure}[h]
% selecting unit length
\centering
% used for centering Figure
\setlength{\unitlength}{0.3cm}
\begin{picture}(40,20)(0,0)
\multiput(5,15)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{3.7}}
\thicklines
\multiput(7,15)(8,0){2}{\line(1,0){4}}
\multiput(20,15)(1,0){4}{\circle*{0.3}}
\multiput(5,13)(8,0){2}{\line(0,-1){4}}
\put(24,15){\line(1,0){4}}
\put(30,13){\line(0,-1){4}}
\thinlines
\put(30,15){\circle{4}}
\put(30,7){\circle{4}}
\put(30,7){\circle{3.7}}
\put(3.5,14.6){$\phi(X_1)$}
\put(11.5,14.6){$\phi(X_2)$}
\put(28.5,14.6){$\phi(X_n)$}
\put(3.5,6.6){$\phi(O_1)$}
\put(11.5,6.6){$\phi(O_2)$}
\put(28.5,6.6){$\phi(O_n)$}
\put(7,16.5){\footnotesize{$\psi(X_1{,}X_2)$}}
\put(15,16.5){\footnotesize{$\psi(X_2{,}X_3)$}}
\put(22.5,16.5){\footnotesize{$\psi(X_{n{-}1}{,}X_n)$}}
\put(5.2,10.6){\footnotesize{$\psi(X_1{,}O_1)$}}
\put(13.2,10.6){\footnotesize{$\psi(X_2{,}O_2)$}}
\put(30.2,10.6){\footnotesize{$\psi(X_n{,}O_n)$}}
\end{picture}
\label{fig:CMRF}
\caption{A chain structured Markov Random Field. Probability of a sequence assignment is defined as a function of the node and edge potentials}
\end{figure}

\pagebreak

The probability of the a sequence assignment is defined as follows :
\begin{align*}
P(X_1,X_2,\dotsc,X_n) &= \frac{1}{Z}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})} \\
\text{where} \quad Z &= \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\end{align*}

%\includegraphics[width=\textwidth]{dummy.pdf}

$Z$ is a normalizing constant known as the partition function. The value of $Z$ can be calculated efficiently using the sum-product algorithm. To calculate the maximum probability assignment $X^*$, we don't even need $Z$. $X^*$ can be evaluated using a dynamic programming formulation known as the max product algorithm. 
\begin{align*}
X^* &= \argmax_{\mathbf{X}}P(\mathbf{X})\\
 &= \argmax_{X_1,X_2,\dotsc,X_n}\frac{1}{Z}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})} \\
 &= \argmax_{X_1,X_2,\dotsc,X_n}\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O_i})\psi(\mathbf{O_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}
\end{align*}

 The main steps in the max-product algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    \phi(\mathbf{O_1})\psi(\mathbf{O_1},X_1{=}k)\phi(X_1{=}k) & \quad \text{if $i=1$}\\
    \phi(\mathbf{O_i})\phi(X_i{=}k)\psi(\mathbf{O_i},X_i{=}k)\max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum functional value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Multistate protein design}
The problem of protein structure prediction is finding the lowest energy structure given a particular sequence. Protein design is known as the inverse protein structure prediction problem and involves finding the lowest energy sequence for a given structure. Multistate protein design generalizes protein design by asking to find a sequence that is stable for multiple structures. This is often posed as a  multi-objective optimization problem~\cite{Zheng2009}.

\paragraph*{} Suppose, $E_A(X)$ and $E_B(X)$ are the energies of a sequence in state A and state B respectively. Then one way of doing multistate design would be to find $X^* = \arg \min_{X} E_A(X) + E_B(X)$. This would be the sequence that is likely to be stable under both state A and state B. Building on this idea, we might prefer to find a sequence $X^*$ that is more stable in one form than the other. We can thus solve for $X^* = \arg \min_{X} \theta_AE_A(X) + \theta_BE_B(X)$ where $\theta_A+\theta_B=1$ and $\theta_A,\theta_B \geq 0$. This sequence might be different from the solution to the previous optimization problem. By trying different combinations of $\theta_A$ and $\theta_B$ it is possible to find a number of different low energy sequences. However, it is not possible to try all possible values of $\theta$, nor is it possible to know which ones to pick to get a variety of different sequences.

\paragraph*{} We will present an algorithm for finding optimal sequences for all combinations of $\theta_A$ and $\theta_B$. This is known as the pareto optimal frontier of the sequence space. See Figure~\ref{fig:cvxfront} for an illustration of the pareto-optimal frontier. This frontier is pareto-optimal in the sense that it is not possible to unilaterally improve the energy function value of one state without adversely affecting the energy value of the other state. Thus each sequence on the frontier is optimal for certain sets of $\theta_A$ and $\theta_B$. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/convexfront.pdf}
    \caption{This figure illustrates the concept of a pareto-optimal convex frontier. The red curve is defined as the pareto-frontier and represents the tradeoff between $E_A(X)$ and $E_B(X)$. P1 and P2 are two points on the pareto frontier. P1 is better in terms of Energy A and P2 is better in terms of Energy B but not vice versa, thus showing a trade-off. The region D2(blue rectangle) is strictly dominated by P2, the region D1(green rectangle) is dominated by P1 and the region D12(yellow) is dominated by both P1 and P2. Domination means that points in these regions are clearly worse in both Energy A and Energy B. Also, notice the perpendicular drawn from the green dashed line and blue dashed line meets the pareto-frontier at P1 and P2 respectively.}
    \label{fig:cvxfront}
\end{figure}

\pagebreak

\subsubsection*{Quick hull procedure}
The Quick hull procedure is used to find all the points on the pareto optimal frontier. The quick hull is traditionally used to find the convex hull of a set of points. In our case we are only interested in the lower left convex frontier similar to ~\cite{Zheng2009}.  The algorithm in pseudo code is described in Algorithm~\ref{algo:quickhull1}. The basic idea behind the algorithm is to find a pair of points on the pareto optimal frontier. Find the point farthest from the line joining the two points and then to repeat the procedure with this newly found point and the previous two points. When there are no newer points to find, all points on the pareto optimal frontier have been discovered. 

\begin{algorithm}
\caption{Quick Hull} \label{algo:quickhull1}
\begin{algorithmic}
\State Initialize queue $Q \gets \emptyset$
\State Find $X_A \gets \arg \min_X E_A(X) $
\State Find $X_B \gets \arg \min_X E_B(X) $
\State Enqueue $(X_A,X_B)$
\State $C_H \gets \{X_A,X_B\}$
\Repeat
	\State Dequeue from Q a pair $(X_1,X_2)$ and assert($E_A(X_1) < E_A(X_2)$)
	\State $m \gets \frac{E_B(X_1)-E_B(X_2)}{E_A(X_1)-E_A(X_2)}$
	\State $\theta_A \gets \frac{-m}{1-m}$
	\State $\theta_B \gets \frac{1}{1-m}$
	\State Find $X_{AB} \gets \arg \min_X \theta_A E_A(X) + \theta_B E_B(X) $
	\If{$X_{AB} \neq X_A$ and $X_{AB} \neq X_B$ }
		\State $C_H \gets C_H \cup \{X_{AB}\}$
		\State Enqueue $(X_A,X_{AB})$ and $(X_{AB},X_B)$
	\EndIf
\Until{Q is empty}
\State \Return $C_H$
\end{algorithmic}
\end{algorithm}


\subsection{Convex combinations of CMRFs}
For multi-state protein design we can combine the energy functions as follows.
\[
E_{AB}(X) = \theta_A E_A(X) + \theta_B E_B(X) \quad \text{where} \quad \theta_A,\theta_B \geq 0 \text{ and } \theta_A + \theta_B =1
\]
Here $E_A(X)$ is any energy function mapping sequence to energy values for state A. The lower the energy the better the fit for sequence X to state A. $E_A(X)$ can also be written as a CMRF.
\[
E_A(X) = -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
Note that $-\log$ is to formulate the problem as a minimization problem so that it agrees with the energy concept. However the optimal sequence remain the same. 
\[
\begin{split}
X_A &= \arg \min_X -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]\\
&= \arg \max_X \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\end{split}
\]
Taking the $-\log$ also offers benefits as it lets us combine convex combinations of CMRF energy functions and gives rise to new graphical models called Tree MRFS (TMRFs). These TMRFs are conceptually very similar to CMRFs and possess the properties of efficient inference and decoding. 

\begin{align*}
X_A &= \arg \min_X E_A(X) \\
&= \arg \min_X -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]\\
&= \arg \min_X -\left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\phi(\mathbf{O^A_i})+\sum_{i=1}^{n} \log\psi(\mathbf{O^A_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right] \\
&= \arg \min_X -\left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^A_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right]
\end{align*}

Similarly,
\begin{align*}
X_B &= \arg \min_X E_B(X) \\
&= \arg \min_X -\left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^B_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right]
\end{align*}
\\
Taking the convex combination of the energy functions we get 
\begin{align*}
X_{AB} &= \arg \min_X \theta_A E_A(X) + \theta_B E_B(X) \\
\text{s.t} &\quad  \theta_A + \theta_B  = 1 \\
& \theta_A, \theta_B \geq 0
\end{align*}
Expanding $E_A(X)$ and $E_B(X)$ we get, 
\begin{align*}
X_{AB} &= \arg \min_X -\theta_A \left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^A_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right] \\
&-\theta_B \left[{\sum_{i=1}^{n} \log\phi(X_i)+\sum_{i=1}^{n} \log\psi(\mathbf{O^B_i},X_i)+ \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}}\right] \\
&= \arg \min_X -\left( \sum_{i=1}^{n}{\log\phi(X_i)} + \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}\right)\\ 
 &-\left( \sum_{i=1}^{n}{(\theta_A\log\psi(\mathbf{O^A_i},X_i) + \theta_B\log\psi(\mathbf{O^B_i},X_i))} \right) \\
 &= \arg \min_X -\left( \sum_{i=1}^{n}{\log\phi(X_i)} + \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}\right)\\ 
 &-\left( \sum_{i=1}^{n}{(\log\psi(\mathbf{O^A_i},X_i)^{\theta_A} + \log\psi(\mathbf{O^B_i},X_i)^{\theta_B})} \right)
\end{align*}

This looks very much like decoding on a CMRF with the added change that the emission edge potential values are exponentiated. In fact this distribution factorizes according to the TMRF shown in Figure~\ref{fig:CMRF2}.



\begin{figure}[h!]
% selecting unit length
\centering
% used for centering Figure
\setlength{\unitlength}{0.3cm}
\begin{picture}(40,25)(0,0)
\multiput(5,15)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{4}}
\multiput(5,7)(8,0){2}{\circle{3.7}}
\multiput(5,23)(8,0){2}{\circle{4}}
\multiput(5,23)(8,0){2}{\circle{3.7}}
\thicklines
\multiput(7,15)(8,0){2}{\line(1,0){4}}
\multiput(20,15)(1,0){4}{\circle*{0.3}}
\multiput(5,13)(8,0){2}{\line(0,-1){4}}
\multiput(5,17)(8,0){2}{\line(0,1){4}}
\put(24,15){\line(1,0){4}}
\put(30,13){\line(0,-1){4}}
\put(30,17){\line(0,1){4}}
\thinlines
\put(30,15){\circle{4}}
\put(30,7){\circle{4}}
\put(30,7){\circle{3.7}}
\put(30,23){\circle{4}}
\put(30,23){\circle{3.7}}
\put(3.5,14.6){$\phi(X_1)$}
\put(11.5,14.6){$\phi(X_2)$}
\put(28.5,14.6){$\phi(X_n)$}
\put(3.5,6.6){$\phi(O^A_1)$}
\put(11.5,6.6){$\phi(O^A_2)$}
\put(28.5,6.6){$\phi(O^A_n)$}
\put(3.5,22.6){$\phi(O^B_1)$}
\put(11.5,22.6){$\phi(O^B_2)$}
\put(28.5,22.6){$\phi(O^B_n)$}
\put(7,16.5){\footnotesize{$\psi(X_1{,}X_2)$}}
\put(15,16.5){\footnotesize{$\psi(X_2{,}X_3)$}}
\put(22.5,16.5){\footnotesize{$\psi(X_{n{-}1}{,}X_n)$}}
\put(5.2,10.6){\footnotesize{$\psi^{\theta_A}(X_1{,}O^A_1)$}}
\put(13.2,10.6){\footnotesize{$\psi^{\theta_A}(X_2{,}O^A_2)$}}
\put(30.2,10.6){\footnotesize{$\psi^{\theta_A}(X_n{,}O^A_n)$}}
\put(5.2,18.6){\footnotesize{$\psi^{\theta_B}(X_1{,}O^B_1)$}}
\put(13.2,18.6){\footnotesize{$\psi^{\theta_B}(X_2{,}O^B_2)$}}
\put(30.2,18.6){\footnotesize{$\psi^{\theta_B}(X_n{,}O^B_n)$}}
\end{picture}
\label{fig:CMRF2}
\caption{A Tree structured Markov Random Field that is used for modelling multi-state protein design. This Tree structured MRF results from taking a convex combination $\theta_A E_A(X) + \theta_B E_B(X)$ of the log likelihoods of CMRFs for State A and State B }
\end{figure}

\pagebreak

\subsection{Max-Decoding on TMRFs}
In the previous section we showed how the convex combination of negative log likelihoods of CMRFs can be used to construct a TMRF. Here we will show how efficient decoding can be performed on the TMRF using the similar variant of Viterbi decoding.  

\begin{align*}
X_{AB} &= \argmin_X \left( \theta_A E_A(X) + \theta_B E_B(X)\right) \\
&= \arg \min_X \left( -\sum_{i=1}^{n}{\log\phi(X_i)} + \sum_{i=1}^{n-1}{\log\psi(X_i,X_{i+1})}\right)\\ 
 &-\left( \sum_{i=1}^{n}{(\log\psi(\mathbf{O^A_i},X_i)^{\theta_A} + \log\psi(\mathbf{O^B_i},X_i)^{\theta_B})} \right) \\
 &= \argmax_{X}\log\left( \prod_{i=1}^{n}{\phi(X_i)\psi(\mathbf{O^A_i},X_i)^{\theta_A} \psi(\mathbf{O^B_i},X_i)^{\theta_B}}\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}\right) \\
 &= \argmax_{X}\left( \prod_{i=1}^{n}{\phi(X_i)\psi(\mathbf{O^A_i},X_i)^{\theta_A} \psi(\mathbf{O^B_i},X_i)^{\theta_B}}\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}\right)
\end{align*}

 The main steps in the max-product algorithm are : 

\vspace*{0.2in}

\textit{Calculation of the Optimal Probability value:}
\[
  V(i,k) = \left\{
  \begin{array}{l l}
    \psi(\mathbf{O^A_1},X_1{=}k)^{\theta_A}\psi(\mathbf{O^B_1},X_1{=}k)^{\theta_B}\phi(X_1{=}k) & \quad \text{if $i=1$}\\
    \phi(X_i{=}k)\psi(\mathbf{O^A_i},X_i{=}k)^{\theta_A}\psi(\mathbf{O^B_1},X_1{=}k)^{\theta_B}\cdots\\
    \max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} & \quad \text{if $2\leq i\leq n$}\\
  \end{array} \right.
\]


\textit{Retrieval of the optimal sequence corresponding to the optimal probability value:}
\begin{align*}
X^\star_n &= \argmax_{k}V(n,k) \\
X^\star_{i} &= Ptr(X^\star_{i+1},i+1) & \quad \text{if $1\leq i \leq n-1 $}
\end{align*}

Here, $V(i,k)$ corresponds to the maximum functional value attainable by examining the sequence from $X_{1:i}$ and the observations from $\mathbf{O^A_{1:i}}$ and $\mathbf{O^B_{1:i}}$. Also, $X^\star_{1:n}$ corresponds to the optimal sequence assignment. Pointers are stored to each of the previous variables during the calculation of $V(i,k)$. These pointers are used to retrieve the optimal sequence assignment. 

\subsection{Convex Hull Procedure on TMRFs}
In the previous section we showed how to find $X_{AB} = \argmin_X \left( \theta_A E_A(X) + \theta_B E_B(X)\right)$, for any convex combination with $\theta_A$ and $\theta_B$.In this section, we will put it all together in Algorithm~\ref{algo:pareto1} and Algorithm~\ref{algo:pareto2} to define a comprehensive algorithm that can be used for finding the entire pareto frontier for sequence design tasks.

\begin{algorithm}
\caption{Pareto-Optimal MultiState Protein Design} \label{algo:pareto1}
\begin{algorithmic}[1]
\Procedure{Pareto-Frontier}{$\phi,\psi,O^A,O^B$}\Comment{Find Pareto frontier}
	\State Initialize queue $Q \gets \emptyset$
	\State $X_A \gets$ \Call{CMRF-Decode}{$\phi,\psi,O^A$}  \Comment{$\argmin_X E_A(X)$}
	\State $X_B \gets$ \Call{CMRF-Decode}{$\phi,\psi,O^B$}  \Comment{$\argmin_X E_B(X)$}
	\State Enqueue $(X_A,X_B)$
	\State $C_H \gets \{X_A,X_B\}$
	\Repeat
		\State Dequeue from Q a pair $(X_1,X_2)$ and assert($E_A(X_1) < E_A(X_2)$)
		\State $m \gets \frac{E_B(X_1)-E_B(X_2)}{E_A(X_1)-E_A(X_2)}$
		\State $\theta_A \gets \frac{-m}{1-m}$
		\State $\theta_B \gets \frac{1}{1-m}$
		\State $X_{AB} \gets $\Call{TMRF-Decode}{$\phi,\psi,O^A,O^B,\theta_A,\theta_B$}
		\Statex \Comment{$\argmin_X \theta_A E_A(X) + \theta_B E_B(X)$}		
		\If{$X_{AB} \neq X_A$ and $X_{AB} \neq X_B$ }
			\State $C_H \gets C_H \cup \{X_{AB}\}$
			\State Enqueue $(X_A,X_{AB})$ and $(X_{AB},X_B)$
		\EndIf
	\Until{Q is empty}
	\State \Return $C_H$
\EndProcedure
\Statex 
\Procedure{CMRF-Decode}{$\phi,\psi,O^A$}\Comment{Max decode CMRF}
	\State Allocate $V[N][K]$ \Comment{N is length of seq,K is numAA }
	\State Allocate $X_A[N]$	
	\For{$k \gets 1,K$}
		\State $V[1][k] \gets \psi(\mathbf{O^A_1},X_1{=}k)\phi(X_1{=}k)$
	\EndFor	
	\For{$i \gets 1,N$}
		\For{$k \gets 1,K$}
			\State $V[i][k] \gets \phi(X_i{=}k)\psi(\mathbf{O^A_i},X_i{=}k) \max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\}$
			\State $Ptr[i][k] \gets \argmax_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} $
		\EndFor		
	\EndFor
	\Statex \Comment{Now retrieve the pointers}
	\State $X_A[N] \gets \argmax_{k}V[N][k]$ 
	\For{$i \gets N-1,1$}
		\State	$X_A[i] \gets Ptr[i+1][X_A[i+1]]$
	\EndFor
	\State \bf{return} $X_A$
\EndProcedure
\algstore{paretobreak}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pareto continued} \label{algo:pareto2}
\begin{algorithmic}[1]
\algrestore{paretobreak}

\Procedure{TMRF-Decode}{$\phi,\psi,O^A,O^B,\theta_A,\theta_B$}\Comment{Max Decode TMRF}
	\State Allocate $V[N][K]$ \Comment{N is length of seq,K is numAA }
	\State Allocate $X_{AB}[N]$	
	\For{$k \gets 1,K$}
		\State $V[1][k] \gets \psi(\mathbf{O^A_1},X_1{=}k)^{\theta_A}\psi(\mathbf{O^B_1},X_1{=}k)^{\theta_B}\phi(X_1{=}k)$
	\EndFor	
	\For{$i \gets 1,N$}
		\For{$k \gets 1,K$}
			\State $V[i][k] \gets \phi(X_i{=}k)\psi(\mathbf{O^A_i},X_i{=}k)^{\theta_A}\psi(\mathbf{O^B_1},X_1{=}k)^{\theta_B}
    \max_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\}$
			\State $Ptr[i][k] \gets \argmax_{j}\{\psi(X_{i}{=}k,X_{i-1}{=}j)V(i-1,j)\} $
		\EndFor		
	\EndFor
	\Statex \Comment{Now retrieve the pointers}
	\State $X_{AB}[N] \gets \argmax_{k}V[N][k]$ 
	\For{$i \gets N-1,1$}
		\State	$X_{AB}[i] \gets Ptr[i+1][X_{AB}[i+1]]$
	\EndFor
	\State \bf{return} $X_{AB}$
\EndProcedure

\end{algorithmic}
\end{algorithm}

\pagebreak


\subsection{Parameter Learning/Training}
So far, we have assumed that the $\phi,\psi$ parameters were available to us. In this section we will describe how these weights are learned. How we can use these weights to for defining the joint TMRF. 


\subsubsection{Training an HMM}


\subsubsection{Converting a HMM to CMRF}
It is possible to reparameterize an HMM so that it now becomes a CMRF. The partition function $Z$ of the newly constructed CMRF equals 1 corresponding to a probability distribution., However there is 

\subsection{Combining the CMRF to a HMM}




\section{Experiments}
In this section we evaluate the performance of our algorithm on simulated as well as experimental data. 

\subsection{Simulation Experiments}
We demonstrate how our algorithm performs on simulated data. The data was prepared by considering a protein of length 15. The sequence space was limited to an alphabet consisting of two arbitrary characters $\alpha,\beta$. The observation belong to a discrete space Helix, Beta and Loop which represented by H,B and L. The weights of the CMRF have weights $\phi,\psi$ as defined in Tables~\ref{tab:psi1} and~\ref{tab:psi2}. We define two states State A and State B that corresponding to competing objectives for multistate design. State A is a Helix-Loop-Helix strand whereas State B is Sheet-Loop-Sheet strand. Figure~\ref{fig:simstates} shows a cartoon diagrams for our design problem. State A and State B have \texttt{HHHHLLLHHHH} and \texttt{BBBBLLLBBBB} secondary structure features respectively. These are the observations for the CMRF.
\\
\\
Using the weight matrices we defined in Tables~\ref{tab:ps1} and~\ref{tab:psi2} we run CMRF-Decode to get $X_A$ which satisfies $X_A = \argmin_X E_A(X)$. We obtain $X_B$ similarly. Not surprisingly $X_A$ is $\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha\alpha$ 
and $X_B$ is $\beta\beta\beta\beta\beta\beta\beta\beta\beta\beta\beta\beta\beta\beta$. This is because the weights in the table are biased towards $\alpha$ emitting \texttt{H} and $\beta$ emitting \texttt{B}. Thus according to our model $X_A$ is the sequence that is most stable under state A, whereas $X_B$ is the sequence is most stable under state B.
\\
\\
We then run the \texttt{Pareto-Frontier} algorithm using the same weights and observations for state A and state B to enumerate the entire pareto frontier. We number the points to show the path taken by the pareto frontier algorithm. We also calculate the energies of all the $2^15$ sequences possible under this restricted sequence model and plot the points. The \texttt{Pareto-Frontier} algorithm was able to find the pareto frontier in \textcolor{red}{N steps} steps. See Figure~\ref{fig:pareto-sim} for the trace taken by the algorithm.  We also compare the running times as opposed to brute force and MCMC and compare the running times. 



\begin{table}
\begin{center}
\begin{tabular}{cc|c|c|c|}
\cline{3-5}
& & \multicolumn{3}{c|}{Observations($\mathbf{O_i}$)} \\ \cline{3-5}
& & Helix(H) & Beta(B) & Loop(L) \\ \cline{1-5}
\multicolumn{1}{|c|}{\multirow{2}{*}{Sequence($X_i$)}} &
\multicolumn{1}{|c|}{$\alpha$} & 0.6 & 0.2 & 0.2      \\ \cline{2-5}
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{$\beta$} & 0.2 & 0.6 & 0.2      \\ \cline{1-5}
\end{tabular}
\end{center}
\caption{$\psi(X,\mathbf{O})$ contains the weights for edge potentials of sequence and features together }
\label{tab:psi1}
\end{table}


\begin{table}
\begin{center}
\begin{tabular}{cc|c|c|}
\cline{3-4}
& & \multicolumn{2}{c|}{Sequence($X_{i+1}$)} \\ \cline{3-4}
& & $\alpha$ & $\beta$  \\ \cline{1-4}
\multicolumn{1}{|c|}{\multirow{2}{*}{Sequence($X_i$)}} &
\multicolumn{1}{|c|}{$\alpha$} & 0.7 & 0.3      \\ \cline{2-4}
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{$\beta$} & 0.3 & 0.7      \\ \cline{1-4}
\end{tabular}
\end{center}
\caption{$\psi(X_i,X_{i+1})$ contains the weights for the edge potentials of adjacent amino acids}
\label{tab:psi2}
\end{table}



\begin{table}
\begin{center}
\begin{tabular}{cc|c|c|}
\cline{3-4}
& & \multicolumn{2}{c|}{Sequence($X_{i+1}$)} \\ \cline{3-4}
& & $\alpha$ & $\beta$  \\ \cline{1-4}
\multicolumn{1}{|c|}{\multirow{2}{*}{Sequence($X_i$)}} &
\multicolumn{1}{|c|}{$\alpha$} & 0.7 & 0.3      \\ \cline{2-4}
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{$\beta$} & 0.3 & 0.7      \\ \cline{1-4}
\end{tabular}
\end{center}
\caption{Time and steps taken on simulated data}
\end{table}

\section{Discussion}


\subsubsection{Normalizing CMRFs}
So far we have been assuming that the $E_A(X)$ and $E_B(X)$ lies in the positive quadrant. However this assumption need not not be true under our CMRF model. This is because it is possible that $f(X) > 1$ and conversely $-\log f(X) < 0$. This can lead to the energy function values being negative as shown in Figure~\ref{fig:cvxfront2}. We can potentially fix this be normalizing energy the Likelihood function with the partition function Z. The partitions can be efficiently calculated using the sum-product algorithm for trees. However we will show that this step is not needed. 

For e.g. let the partition functions for each of the states be.
\[
Z_A = \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
and 
\[
Z_B = \sum_{X_1,X_2,\dotsc,X_n}\left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^B_i})\psi(\mathbf{O^B_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
\\
If instead of the regular energy function 
\[
E_A(X) = -\log  \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]
\]
we normalized the energy function with $Z_A$
\[
\begin{split}
E_A'(X) &= -\log  \frac{1}{Z_A} \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right]\\
 &= -\log   \left[{\prod_{i=1}^{n}{\phi(X_i)\phi(\mathbf{O^A_i})\psi(\mathbf{O^A_i},X_i})\prod_{i=1}^{n-1}{\psi(X_i,X_{i+1})}}\right] + \log Z_A \\
	&= E_A(X) + \log Z_A
\end{split}
\]
\\
Then optimizing the multistate Energy function would amount to : 
\[
\begin{split}
X^{\dagger} &= \arg \min_X \left( \theta_A E_A'(X)  + \theta_B E_B'(X) \right) \\
 &= \arg \min_X \left( \theta_A E_A(X) + \theta_B E_B(X) + \theta_A \log Z_A + \theta_B \log Z_B \right) \\
 &= \arg \min_X \left( \theta_A E_A(X) + \theta_B E_B(X) \right) \\
  &= X^*
\end{split}
\]

This shows that normalizing the energy function values by $Z_A$ and $Z_B$ is not necessary in order to find point on the pareto optimal frontier. This is further demonstrated in Figure~\ref{fig:cvxfront2}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/convexfront2.pdf}
    \caption{This figure illustrates the issue with normalizing. When dealing with negative log likelihood values, it is possible that pareto frontier may not lie in the positive quadrant. This however does not imply that the method fails. Normalizing with the partition function merely corresponds to translating the points in the energy space. This transformation keeps the optimal solution invariant.  }
    \label{fig:cvxfront2}
\end{figure}
\pagebreak


\subsubsection{Clarification on Energy Convex Combination}
From the discussion in the beginning, the reader may have been lead to believe that we are taking convex combinations of the likelihood of multiple states and then maximizing this likelihood. While in spirit this is true, however practically we are optimizing something else. Let $L(X|O^A)$  and $L(X|O^B)$ be the likelihoods of the sequences under state A and state B respectively. The reader may have been lead to believe that we are solving the following problem
\begin{equation}
\label{eqn:clarif1}
X^* = \arg \max_X \left( \theta_A L(X|O^A) + \theta_B L(X|O^B) \right)
\end{equation}
But in, reality we are solving : 
\begin{align}
X^* &= \arg \min_X \left( -\theta_A \log L(X|O^A) - \theta_B \log L(X|O^B) \right) \\
&= \arg \min_X \left( \log \frac{1}{L(X|O^A)^{\theta_A} L(X|O^B)^{\theta_B}} \right) \\
&= \arg \max_X \left( L(X|O^A)^{\theta_A} L(X|O^B)^{\theta_B} \right)
\label{eqn:clarif2} 
\end{align}

Clearly Eqn~\ref{eqn:clarif2} is not the same as Eqn~\ref{eqn:clarif1}. One is minimizing the arithmetic mean, while the other is minimizing the geometric mean. It remains to be seen what the semantic consequence of this mathematical detail is. 

\section{Conclusion}
We presented a method that is able to perform fast decoding and inference for finding minimum energy sequences for the purpose of protein design. We also show to find the pareto-frontier for the sequence space. The key idea behind this method is the use of Tree MRFs which allow fast Dynamic Programming for max-decoding.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}